{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hackathon/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse, os, csv, numpy as np\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Dense, Dropout, Input, Flatten, GlobalAveragePooling2D, BatchNormalization, Conv2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.losses import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = '../data/train_image'\n",
    "val_data = '../data/val_image'\n",
    "train_label = '../data/coordinates/translated_coords.csv'\n",
    "val_label = '../data/coordinates/translated_coords_val.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    input_x = Input(shape=(224, 224, 3))\n",
    "    first_layer = Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                     activation='relu')(input_x)\n",
    "    first_layer = BatchNormalization()(first_layer)\n",
    "    first_layer = Dropout(0.5)(first_layer)\n",
    "    \n",
    "    second_layer = Conv2D(64, (5,5), activation='relu')(first_layer)\n",
    "    second_layer = GlobalAveragePooling2D()(second_layer)\n",
    "    second_layer = Dropout(0.5)(second_layer)\n",
    "    \n",
    "    first_fc = Dense(2)(second_layer)\n",
    "    second_fc = Dense(2)(second_layer)\n",
    "    third_fc = Dense(2)(second_layer)\n",
    "    fourth_fc = Dense(2)(second_layer)\n",
    "    \n",
    "    model = Model(inputs=input_x, outputs=[first_fc, second_fc, third_fc, fourth_fc])\n",
    "    sgd = optimizers.SGD(lr=0.001, clipvalue=0.5)\n",
    "    model.compile(loss=mean_squared_error, optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(label):\n",
    "    with open(label, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        filenames = [f[0] for f in csv_reader]\n",
    "    shuffle(filenames)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_read(data, label, filenames):\n",
    "    x, y1, y2, y3, y4, file_index = [], [], [], [], [], []\n",
    "    feature1, feature2, feature3, feature4 = [], [], [], []\n",
    "    with open(label, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            file_index.append(row[0])\n",
    "            feature1.append(row[1:3])\n",
    "            feature2.append(row[3:5])\n",
    "            feature3.append(row[5:7])\n",
    "            feature4.append(row[7:])\n",
    "\n",
    "    for file in filenames:\n",
    "        input_path = os.path.join(data, file)\n",
    "        im = np.asarray(Image.open(input_path))\n",
    "        x.append(im)\n",
    "\n",
    "        idx = file_index.index(file)\n",
    "        y1.append(feature1[idx])\n",
    "        y2.append(feature2[idx])\n",
    "        y3.append(feature3[idx])\n",
    "        y4.append(feature4[idx])\n",
    "\n",
    "    return np.array(x), np.array(y1), np.array(y2), np.array(y3), np.array(y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(batch_size, data, label):\n",
    "    filenames = get_filenames(label)\n",
    "    while 1:\n",
    "        X, y1, y2, y3, y4 = [], [], [], [], []\n",
    "        shuffle(filenames)\n",
    "        for i in range(41):\n",
    "            files_to_select = filenames[i*batch_size:(i+1)*batch_size]\n",
    "            one_x, one_y1, one_y2, one_y3, one_y4 = data_read(data, label, files_to_select)\n",
    "            X.append(one_x)\n",
    "            y1.append(one_y1)\n",
    "            y2.append(one_y2)\n",
    "            y3.append(one_y3)\n",
    "            y4.append(one_y4)\n",
    "        y = [y1, y2, y3, y4]\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 41\n",
      "41 41\n",
      "41 41\n"
     ]
    }
   ],
   "source": [
    "tg = generator(10, train_data, train_label)\n",
    "for e in range(3):\n",
    "    ex, ey = next(tg)\n",
    "    print(len(ex), len(ey[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['dense_4_acc'])\n",
    "    plt.plot(history.history['val_dense_4_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['dense_4_acc', 'val_dense_4_acc'], loc='upper left')\n",
    "    fig = plt.figure()\n",
    "    fig.savefig('../visual/accuracy.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['dense_1_acc'])\n",
    "    plt.plot(history.history['val_dense_1_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['dense_1_acc', 'val_dense_1_acc'], loc='upper left')\n",
    "    fig = plt.figure()\n",
    "    fig.savefig('../visual/accuracy.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['dense_2_acc'])\n",
    "    plt.plot(history.history['val_dense_2_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['dense_2_acc', 'val_dense_2_acc'], loc='upper left')\n",
    "    fig = plt.figure()\n",
    "    fig.savefig('../visual/accuracy.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['dense_3_acc'])\n",
    "    plt.plot(history.history['val_dense_3_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['dense_3_acc', 'val_dense_3_acc'], loc='upper left')\n",
    "    fig = plt.figure()\n",
    "    fig.savefig('../visual/accuracy.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['dense_4_loss'])\n",
    "    plt.plot(history.history['val_dense_4_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['dense_4_loss', 'val_dense_4_loss'], loc='upper left')\n",
    "    fig_loss = plt.figure()\n",
    "    fig_loss.savefig('../visual/loss.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    batch_size = 10\n",
    "    #tensorboard = Tensorboard(log_dir = '../logs')\n",
    "    \n",
    "    filepath=\"../model_ckpts/weights-improvement-{epoch:02d}-{val_dense_1_acc:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_dense_1_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    train_files = get_filenames(train_label)\n",
    "    val_files = get_filenames(val_label)\n",
    "    \n",
    "    train_x, y1, y2, y3, y4 = data_read(train_data, train_label, train_files)\n",
    "    val_x, val_y1, val_y2, val_y3, val_y4 = data_read(val_data, val_label, val_files)\n",
    "    \n",
    "    train_gen = generator(batch_size, train_data, train_label)\n",
    "    val_gen = generator(batch_size, val_data, val_label)\n",
    "    \n",
    "    history = model.fit_generator(train_gen, steps_per_epoch=int(410/batch_size), epochs=100, verbose=1,\n",
    "                                  callbacks=[checkpoint], validation_data=val_gen,\n",
    "                                    validation_steps=int(178/batch_size))\n",
    "    \n",
    "    visualization(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
